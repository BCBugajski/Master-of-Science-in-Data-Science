{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-14T03:48:02.744528Z",
     "iopub.status.busy": "2022-08-14T03:48:02.743813Z",
     "iopub.status.idle": "2022-08-14T03:48:02.751986Z",
     "shell.execute_reply": "2022-08-14T03:48:02.750517Z",
     "shell.execute_reply.started": "2022-08-14T03:48:02.744492Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import xml.etree.ElementTree as ET \n",
    "from PIL import Image \n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Input, Dense, Reshape, Flatten, Dropout,Concatenate\n",
    "from tensorflow.keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
    "from tensorflow.keras.backend import random_normal, ones_like, zeros_like,mean\n",
    "from tensorflow.keras.layers import UpSampling2D, Conv2D, concatenate\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-14T02:40:34.033664Z",
     "iopub.status.busy": "2022-08-14T02:40:34.033041Z",
     "iopub.status.idle": "2022-08-14T02:40:34.047837Z",
     "shell.execute_reply": "2022-08-14T02:40:34.046563Z",
     "shell.execute_reply.started": "2022-08-14T02:40:34.033627Z"
    }
   },
   "outputs": [],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle Competition: Generation of Dog Images with GAN\n",
    "\n",
    "### By Chase Bugajski\n",
    "\n",
    "## Introduction\n",
    "\n",
    "The goal of a GAN is to \"learn\" the probability density function of a particular data distribution. The data distribution should be specific and diverse; for instance \"trees\" is an excellent choice, because they are distinct from other plants, and there are many subcategories for breadth of distribution. That is to say, the distribution is wide, not narrow. The data we use today comes from a [Kaggle competition](https://www.kaggle.com/competitions/generative-dog-images/overview), and contains many photos of different breeds of dogs. In this project, I will follow along with a few successful notebooks from that competition (now over) to give myself an introduction to the construction of GAN's. I'll try to properly cite everything I'm doing - if I miss something please let me know. There will be a bibliography at the end of this project. Let's import the data, clean it, then do some EDA.\n",
    "\n",
    "## Data Importation and Cleaning\n",
    "\n",
    "The first step in importing the data will be to pull it from kaggle, then unzip the files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-14T03:13:07.680854Z",
     "iopub.status.busy": "2022-08-14T03:13:07.680028Z",
     "iopub.status.idle": "2022-08-14T03:13:27.274545Z",
     "shell.execute_reply": "2022-08-14T03:13:27.273560Z",
     "shell.execute_reply.started": "2022-08-14T03:13:07.680819Z"
    }
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "with zipfile.ZipFile(\"../input/generative-dog-images/all-dogs.zip\",\"r\") as zip_ref:\n",
    "    zip_ref.extractall(\"./data/\")\n",
    "    \n",
    "with zipfile.ZipFile(\"../input/generative-dog-images/Annotation.zip\",\"r\") as zip_ref:\n",
    "    zip_ref.extractall(\"./data/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many of the images contain superfluous objects, such as cans, humans, flowers, etc. We don't want to teach the GAN that the \"dog image\" density function includes these unnessary background objects, because that widens the distribution too much and would require more data for the same performance. By cropping the images to only include the dogs, we can improve the ability of the GAN to generate specifically dogs. If background is desired, just don't crop!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-14T03:13:30.799658Z",
     "iopub.status.busy": "2022-08-14T03:13:30.799302Z",
     "iopub.status.idle": "2022-08-14T03:15:09.401648Z",
     "shell.execute_reply": "2022-08-14T03:15:09.400556Z",
     "shell.execute_reply.started": "2022-08-14T03:13:30.799627Z"
    }
   },
   "outputs": [],
   "source": [
    "dogPics = os.listdir(\"./data/all-dogs/\")\n",
    "breeds = os.listdir(\"./data/Annotation/\")\n",
    "\n",
    "indexIn = 0\n",
    "namesIn = list()\n",
    "#imagesIn will be the new container for cropped images\n",
    "imagesIn = np.zeros((25000, 64, 64, 3))\n",
    "\n",
    "for breed in breeds:\n",
    "    for dog in os.listdir(\"./data/Annotation/\" + breed):\n",
    "        try:\n",
    "            this_dog = Image.open(\"./data/all-dogs/\" + dog + \".jpg\" )\n",
    "        except:\n",
    "            continue\n",
    "        tree = ET.parse(\"./data/Annotation/\" + breed + \"/\" + dog)\n",
    "        root = tree.getroot()\n",
    "        objects = root.findall(\"object\")\n",
    "        for this_ob in objects:\n",
    "            bndbox = this_ob.find('bndbox') \n",
    "            xmin = int(bndbox.find('xmin').text)\n",
    "            ymin = int(bndbox.find('ymin').text)\n",
    "            xmax = int(bndbox.find('xmax').text)\n",
    "            ymax = int(bndbox.find('ymax').text)\n",
    "            w = np.min((xmax - xmin, ymax - ymin))\n",
    "            img2 = this_dog.crop((xmin, ymin, xmin+w, ymin+w))\n",
    "            img2 = img2.resize((64,64), Image.ANTIALIAS)\n",
    "            imagesIn[indexIn,:,:,:] = np.asarray(img2)\n",
    "            if indexIn % 1000==0:  #Optional Loading bar\n",
    "                print(indexIn)\n",
    "            namesIn.append(breed)\n",
    "            indexIn += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-14T03:15:56.686333Z",
     "iopub.status.busy": "2022-08-14T03:15:56.685162Z",
     "iopub.status.idle": "2022-08-14T03:15:57.364012Z",
     "shell.execute_reply": "2022-08-14T03:15:57.363002Z",
     "shell.execute_reply.started": "2022-08-14T03:15:56.686281Z"
    }
   },
   "outputs": [],
   "source": [
    "# Shuffle dog pics so discriminator doesn't learn to associate dog breeds with index\n",
    "index = np.arange(indexIn)\n",
    "np.random.shuffle(index)\n",
    "imagesIn = imagesIn[index,:,:,:]\n",
    "namesIn = np.array(namesIn)[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-14T03:15:59.048472Z",
     "iopub.status.busy": "2022-08-14T03:15:59.047904Z",
     "iopub.status.idle": "2022-08-14T03:16:00.279829Z",
     "shell.execute_reply": "2022-08-14T03:16:00.278831Z",
     "shell.execute_reply.started": "2022-08-14T03:15:59.048438Z"
    }
   },
   "outputs": [],
   "source": [
    "# Display some cropped images\n",
    "\n",
    "x = np.random.randint(0, indexIn, 25)\n",
    "for k in range(3):\n",
    "    plt.figure(figsize=(15,3))\n",
    "    for j in range(5):\n",
    "        plt.subplot(1,5,j+1)\n",
    "        img = Image.fromarray(imagesIn[x[k*5+j],:,:,:].astype('uint8') )\n",
    "        plt.axis('off')\n",
    "        plt.title(namesIn[x[k*5+j]].split('-')[1],fontsize=11)\n",
    "        plt.imshow(img)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis\n",
    "\n",
    "The following EDA was performed by examining the \"properties\" page of my directories manually. The data supplied by the Kaggle contain 775.22 MB constisting of 20,582 zipped dog images, with 17.62 MB of zipped breed labels. There is exactly one label for one dog image. There were a total of 120 distinct breeds in the data. After unzipping, my all-dogs folder within ./data/ contained 781 MB on disk, and the labels took up 5.3 MB on disk. After cleaning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-14T03:16:14.273080Z",
     "iopub.status.busy": "2022-08-14T03:16:14.272702Z",
     "iopub.status.idle": "2022-08-14T03:16:14.278814Z",
     "shell.execute_reply": "2022-08-14T03:16:14.277602Z",
     "shell.execute_reply.started": "2022-08-14T03:16:14.273047Z"
    }
   },
   "outputs": [],
   "source": [
    "from sys import getsizeof\n",
    "print(\"The cropped dog images have combined data usage of:\", round(getsizeof(imagesIn)/(1e9), 2), \"GB\")\n",
    "print(\"The breed names of cropped dog images have combined data usage of:\", round(getsizeof(namesIn)/(1e6), 2), \"MB\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Construction\n",
    "\n",
    "A GAN really consists of two different bots. A discriminator is a supervised nueral network that is trained on random alterations of real data, whose job is to determine whether a supplied datum is \"real\" or \"fake\". Our discriminator will be fed images of dogs *with intentionally noised up pixels* so that it does not simply memorize the ~20,000 images of dogs. A generator will be a nueral network that generates fake samples to feed into the discriminator. Initially the generator will have random weights, resulting in random Normal noise images. They will look like static, and nothing like dogs. As the discriminator learns to determine static from dogs, so will the generator learn to \"fool\" the discriminator into accepting fake images. There is some super interesting evolutionary game theory that goes into this, I would highly recommend reading material on this! Eventually, we will reach an equilibrium where the discriminator and generator reach a Nash Equilibrium, and the entire system cannot get better at generating new data without additional input images / data. \n",
    "\n",
    "First, I will build a discriminator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-14T03:16:19.145798Z",
     "iopub.status.busy": "2022-08-14T03:16:19.145445Z",
     "iopub.status.idle": "2022-08-14T03:16:19.175243Z",
     "shell.execute_reply": "2022-08-14T03:16:19.174413Z",
     "shell.execute_reply.started": "2022-08-14T03:16:19.145768Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set initial variables\n",
    "num_pixels = 12288\n",
    "in1 = Input((num_pixels, ))\n",
    "noise_size = 10000\n",
    "in2= Input((noise_size, ))\n",
    "batch_size = 64\n",
    "buffer = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-14T03:16:21.112534Z",
     "iopub.status.busy": "2022-08-14T03:16:21.111955Z",
     "iopub.status.idle": "2022-08-14T03:16:23.797409Z",
     "shell.execute_reply": "2022-08-14T03:16:23.796418Z",
     "shell.execute_reply.started": "2022-08-14T03:16:21.112500Z"
    }
   },
   "outputs": [],
   "source": [
    "# Build discriminator,\n",
    "## Citation https://www.kaggle.com/code/roydatascience/introduction-to-generative-adversarial-networks\n",
    "## Citation cdeotte\n",
    "\n",
    "x = Dense(num_pixels, activation = \"sigmoid\")(in2)\n",
    "conc = concatenate([in1, x])\n",
    "x = Reshape((2, num_pixels, 1))(conc)\n",
    "x = Conv2D(filters = 1,\n",
    "            kernel_size = [2, 1],\n",
    "            use_bias = False,\n",
    "            name = 'conv')(x)\n",
    "x = Flatten()(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-14T03:16:26.739587Z",
     "iopub.status.busy": "2022-08-14T03:16:26.739075Z",
     "iopub.status.idle": "2022-08-14T03:16:26.758441Z",
     "shell.execute_reply": "2022-08-14T03:16:26.757441Z",
     "shell.execute_reply.started": "2022-08-14T03:16:26.739544Z"
    }
   },
   "outputs": [],
   "source": [
    "model = x\n",
    "model_discrim = Model([in1, in2], model)\n",
    "\n",
    "#We don't want the discriminator to change it's weights for the\n",
    "## real images, so we will prevent them from training\n",
    "model_discrim.get_layer(\"conv\").trainable = False\n",
    "\n",
    "#We want to set the initial weights for the discriminator\n",
    "initial_discrim_weights = np.array([[[[-1.0 ]]],[[[1.0]]]])\n",
    "model_discrim.get_layer(\"conv\").set_weights([initial_discrim_weights])\n",
    "\n",
    "#Look at a model summary for the discriminator\n",
    "print(\"Here's the model summary for our discriminator:\")\n",
    "model_discrim.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Commentary\n",
    "\n",
    "That's a really big model! There are over 120 million trainable parameters, so we expect the model to run for a very long time before converging. That doesn't even include competition against the generator, which hasn't been construted yet. Before building the generator we first need to train the discriminator on the real dog photos. Let's do that. I did a grid search as hyper-parameter optimization over the learning rate eta. My best result was eta = 0.5. I'll leave in the grid search commented out, but due to resource intensity I don't want to use too much GPU RAM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-14T03:16:32.958704Z",
     "iopub.status.busy": "2022-08-14T03:16:32.958353Z",
     "iopub.status.idle": "2022-08-14T03:18:20.368148Z",
     "shell.execute_reply": "2022-08-14T03:18:20.367119Z",
     "shell.execute_reply.started": "2022-08-14T03:16:32.958675Z"
    }
   },
   "outputs": [],
   "source": [
    "#Compile disciminator. We use loss as binary crossentropy because we want to classify input images as either \"real\" or \"fake\".\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "      initial_learning_rate = 0.5,\n",
    "      decay_steps = 10000,\n",
    "      decay_rate=0.9\n",
    "      )\n",
    "\n",
    "model_discrim.compile(optimizer = tf.keras.optimizers.Adam(learning_rate = lr_schedule),\n",
    "                     loss = \"binary_crossentropy\"\n",
    "                     )\n",
    "\n",
    "#Train the discriminator on the first 10,000 dogs\n",
    "n_samples = 10000\n",
    "train_noise = np.zeros((n_samples, n_samples))\n",
    "for i in range(n_samples):\n",
    "    train_noise[i, i] = 1\n",
    "zeros = np.zeros((n_samples, num_pixels))\n",
    "    \n",
    "#We map the RGB pixel values to [0,1] for stability, and flatten to feed into the discriminator\n",
    "train_imgs = (imagesIn[:n_samples,:,:,:]/255.).reshape((-1, num_pixels)) \n",
    "\n",
    "callbacks_list = [tf.keras.callbacks.EarlyStopping(monitor = \"loss\", patience = 10, mode = \"min\")]\n",
    "\n",
    "#startTime = time()\n",
    "#current_best = (None, 0)\n",
    "#for k in range(5): #Citation roydatascience\n",
    "#    this_history = model_discrim.fit([zeros, train_noise],\n",
    "#                                    train_imgs,\n",
    "#                                    epochs = 100,\n",
    "#                                    batch_size = batch_size,\n",
    "#                                    validation_split = 0.1,\n",
    "#                                    callbacks = callbacks_list,\n",
    "#                                    verbose = 0\n",
    "#                                    )\n",
    "#    print('Epoch',(k+1)*10,'/50 - loss =',h.history['loss'][-1] )\n",
    "#    if this_history.history['loss'][-1]<0.533: lr = 0.1\n",
    "\n",
    "#endTime = time()\n",
    "#print(\"Total grid search time:\", round(endTime - startTime, 2), \"seconds\")\n",
    "\n",
    "discrim_history = model_discrim.fit([zeros, train_noise],\n",
    "                                    train_imgs,\n",
    "                                    epochs = 100,\n",
    "                                    batch_size = batch_size,\n",
    "                                    callbacks = callbacks_list,\n",
    "                                    verbose = 0\n",
    "                                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Commentary\n",
    "\n",
    "Let's visualize the training for the discriminator. We'll graph loss vs. epoch.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-14T03:24:16.706599Z",
     "iopub.status.busy": "2022-08-14T03:24:16.706046Z",
     "iopub.status.idle": "2022-08-14T03:24:59.429985Z",
     "shell.execute_reply": "2022-08-14T03:24:59.428856Z",
     "shell.execute_reply.started": "2022-08-14T03:24:16.706567Z"
    }
   },
   "outputs": [],
   "source": [
    "loss = discrim_history.history[\"loss\"]\n",
    "epochs = [i for i in range(len(loss))]\n",
    "\n",
    "plt.plot(epochs, loss)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Loss Vs. Epoch for Discriminator\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now that we have the discriminator trained, let's go ahead and delete all the training data to conserve RAM, because I'm almost out on kaggle. We will freeze the training weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-14T04:02:18.045762Z",
     "iopub.status.busy": "2022-08-14T04:02:18.045403Z",
     "iopub.status.idle": "2022-08-14T04:02:18.059380Z",
     "shell.execute_reply": "2022-08-14T04:02:18.058226Z",
     "shell.execute_reply.started": "2022-08-14T04:02:18.045732Z"
    }
   },
   "outputs": [],
   "source": [
    "del train_noise, train_imgs, imagesIn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Commentary:\n",
    "\n",
    "We have built a memorizer Discriminator. The discriminator \"knows\" within its weights what a dog looks like. Now all that's left is to build a generator, and to make the discriminator fight it.\n",
    "\n",
    "Let's build a generator. In order to prevent the generator from just learning to feed pixel values that the discriminator expects to see, we will throttle the latent space of the generator. This will force the generator to learn the distribution the discriminator expects rather that specific digits. That is, it will only be able to \"remember\" a finite amount of information - instead of memorizing pixels, it will memorize a function to generate from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-14T04:20:48.733481Z",
     "iopub.status.busy": "2022-08-14T04:20:48.733118Z",
     "iopub.status.idle": "2022-08-14T04:20:48.805800Z",
     "shell.execute_reply": "2022-08-14T04:20:48.804768Z",
     "shell.execute_reply.started": "2022-08-14T04:20:48.733452Z"
    }
   },
   "outputs": [],
   "source": [
    "# Citation cdeotte\n",
    "#We will use UpSampling2D to utilize the algorithm shown in class for image data\n",
    "seed = Input((n_samples,)) \n",
    "x = Dense(2048, activation='elu')(seed)\n",
    "x = Reshape((8,8,32))(x)\n",
    "x = Conv2D(128, (3, 3), activation='elu', padding='same')(x)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "x = Conv2D(64, (3, 3), activation='elu', padding='same')(x)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "x = Conv2D(32, (3, 3), activation='elu', padding='same')(x)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "x = Conv2D(3, (3, 3), activation='linear', padding='same')(x)\n",
    "generated = Flatten()(x)\n",
    "\n",
    "# Reshape output to feed into discriminator\n",
    "reshaped = Reshape((n_samples,))(seed)\n",
    "\n",
    "# Compile generator\n",
    "model_gen = Model(seed, [generated, reshaped])\n",
    "print(\"Here's the summary for the generator:\")\n",
    "model_gen.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Commentary:\n",
    "\n",
    "Another very big model! This is going to take quite some time to train. In the future, I think a wise move would be resize the images to 32 X 32 instead of 64 X 64 to help save on space. Lets connect the generator and discriminator to build ourselves a GAN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-14T04:20:53.025683Z",
     "iopub.status.busy": "2022-08-14T04:20:53.025312Z",
     "iopub.status.idle": "2022-08-14T04:20:53.096100Z",
     "shell.execute_reply": "2022-08-14T04:20:53.094965Z",
     "shell.execute_reply.started": "2022-08-14T04:20:53.025649Z"
    }
   },
   "outputs": [],
   "source": [
    "#Citation cdeotte\n",
    "model_discrim.trainable = False\n",
    "GAN_in = Input(shape = (n_samples, ))\n",
    "x = model_gen(GAN_in)\n",
    "GAN_out = model_discrim(x)\n",
    "\n",
    "#Compile GAN\n",
    "GAN = Model(GAN_in, GAN_out)\n",
    "initial_GAN_weights = np.array([[[[-1 ]]],[[[255.]]]])\n",
    "GAN.get_layer(\"model\").get_layer(\"conv\").set_weights([initial_GAN_weights])\n",
    "\n",
    "GAN.compile(optimizer = \"Adam\",\n",
    "           loss = \"mean_squared_error\"\n",
    "           )\n",
    "\n",
    "GAN.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-14T04:21:26.881593Z",
     "iopub.status.busy": "2022-08-14T04:21:26.880782Z",
     "iopub.status.idle": "2022-08-14T05:02:50.812362Z",
     "shell.execute_reply": "2022-08-14T05:02:50.810832Z",
     "shell.execute_reply.started": "2022-08-14T04:21:26.881551Z"
    }
   },
   "outputs": [],
   "source": [
    "# Citation cdeotte\n",
    "# TRAINING DATA\n",
    "train_noise = np.zeros((n_samples, n_samples))\n",
    "for i in range(n_samples): train_noise[i,i] = 1\n",
    "zeros = np.zeros((n_samples, num_pixels))\n",
    "\n",
    "# TRAIN NETWORKS\n",
    "epochs = 1\n",
    "it = 15\n",
    "lr = 0.005\n",
    "    \n",
    "for k in range(it):  \n",
    "    startTime = time()\n",
    "    annealer = LearningRateScheduler(lambda x: lr)\n",
    "    h = GAN.fit(train_noise,\n",
    "                zeros,\n",
    "                epochs = epochs,\n",
    "                batch_size = 256,\n",
    "                callbacks=[annealer],\n",
    "                verbose=0\n",
    "               )\n",
    "    \n",
    "    print('Epoch',(k+1),'/'+str(it)+' - loss =',h.history['loss'][-1] )\n",
    "    \n",
    "    plt.figure(figsize=(15,3))\n",
    "    \n",
    "    # Show the current generator's ability to make dogs out of noise\n",
    "    for j in range(5):\n",
    "        xx = np.zeros((10000))\n",
    "        xx[np.random.randint(10000)] = 1\n",
    "        plt.subplot(1,5,j+1)\n",
    "        img = model_gen.predict(xx.reshape((-1,10000)))[0].reshape((-1,64,64,3))\n",
    "        img = Image.fromarray( (img).astype('uint8').reshape((64,64,3)))\n",
    "        plt.axis('off')\n",
    "        plt.imshow(img)\n",
    "        \n",
    "    plt.show()  \n",
    "    \n",
    "    # Adjust eta\n",
    "    epochs *= 2\n",
    "    if epochs >= 32:\n",
    "        lr = 0.001\n",
    "    if epochs > 256:\n",
    "        epochs = 256\n",
    "    endTime = time()\n",
    "    print(\"Total epoch training time:\", round(endTime - startTime, 2)/60, \"minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results and Analysis\n",
    "\n",
    "We have successfully trained our generator to make dog images! I ended up running 12 iterations consisting of a varying number of epochs. That's why there's a big red error message that says \"Keyboard Interrupt\". As the number of epochs changed, so did the learning rate to help prevent overfitting. I really can't overstate how wonderful a resource user cdeotte's dog-memorizer-gan notebook was, this project would have been impossible without it. Really all I did was work through his and others tutorials in how to create a GAN in tensorflow and add comments and customization. \n",
    "\n",
    "It's important to note that the generator *did not know* about the different dog breeds, but the discriminator did. As such the generator learned the common features shared by all breeds, as well as the unique features that pertain only to individual breeds. It's really fascinating to watch the generator's understanding of the true \"dog\" pdf evolve. The final model is still imperfect, as can be seen in the pixellation and fuzziness in the output images. What I think is most fascinating about the GAN is it's ability to not only model what you expect it to. The GAN also predicts common backgrounds for dogs! In epoch 11, there is a very convincing image of what appears to be a German Shepherd standing near a street by a park. That's not a German Shepherd, that's not a street, and that's not a park. They are a random collection of pixels that an AI learned would trick another AI into believing the image was \"real\" with respect to a metric (MSE). \n",
    "\n",
    "One important note: it is possible to overfit the data, because we are just trying to make a generator that passes an MSE checker. As the training time increases, the MSE will asymptotically converge to the \"real\" loss of 0, which would indicate we are just replicating the images stored in our Memorizer Discriminator. This is not an issue in a traditional GAN, because the discriminator doesn't \"know\" a priori what a \"dog\" is - it learns in competition with the generator.\n",
    "\n",
    "I'd like to include some visualizations. I think the intermittant outputted dog photos are sufficient to understand what the generator is doing during the GAN training. Let's make a nice visualization regarding MSE and epoch, then submit some fake dogs to kaggle and get a score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-14T05:08:43.562689Z",
     "iopub.status.busy": "2022-08-14T05:08:43.562331Z",
     "iopub.status.idle": "2022-08-14T05:08:43.745747Z",
     "shell.execute_reply": "2022-08-14T05:08:43.744799Z",
     "shell.execute_reply.started": "2022-08-14T05:08:43.562650Z"
    }
   },
   "outputs": [],
   "source": [
    "MSEs = [6605, 2476, 1271, 703, 454, 311, 219, 150, 108, 92, 83, 78]\n",
    "epochs = [i + 1 for i in range(len(MSEs))]\n",
    "\n",
    "plt.plot(epochs, MSEs)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.title(\"GAN MSE Vs. Epoch\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-14T05:28:17.699930Z",
     "iopub.status.busy": "2022-08-14T05:28:17.699543Z",
     "iopub.status.idle": "2022-08-14T05:28:17.707517Z",
     "shell.execute_reply": "2022-08-14T05:28:17.706507Z",
     "shell.execute_reply.started": "2022-08-14T05:28:17.699884Z"
    }
   },
   "outputs": [],
   "source": [
    "#Fake dog image maker\n",
    "def makeDog(seed, index):\n",
    "    # Function makes an 80/20 split between two types of dogs\n",
    "    doggy_zeros = np.zeros((n_samples))\n",
    "    doggy_zeros[index] = 0.80\n",
    "    random_sample = np.random.randint(n_samples)\n",
    "    doggy_zeros[random_sample] = 0.30\n",
    "    this_pic = model_gen.predict(doggy_zeros.reshape((-1, n_samples)))[0]\n",
    "    this_pic = this_pic.reshape((64,64,3))\n",
    "    new_index = (index + 1) % 10000 # Citation cdeotte\n",
    "    dog_pic = Image.fromarray(this_pic.astype(\"uint8\"))\n",
    "    \n",
    "    return (dog_pic, new_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-14T05:33:50.125582Z",
     "iopub.status.busy": "2022-08-14T05:33:50.124847Z",
     "iopub.status.idle": "2022-08-14T05:41:01.745683Z",
     "shell.execute_reply": "2022-08-14T05:41:01.744689Z",
     "shell.execute_reply.started": "2022-08-14T05:33:50.125545Z"
    }
   },
   "outputs": [],
   "source": [
    "# Make some fake dogs to send to Kaggle\n",
    "zippy = zipfile.PyZipFile(\"images.zip\", mode = \"w\")\n",
    "\n",
    "idx = 0\n",
    "for j in range(n_samples):\n",
    "    #Generate a random dog\n",
    "    this_seed = np.random.normal(0, 1, 100)\n",
    "    (this_dog, idx) = makeDog(seed = this_seed, index = idx)\n",
    "    \n",
    "    #Save the dog to a temporary file\n",
    "    fileName = str(j) + \".png\"\n",
    "    this_dog.save(fileName, \"PNG\")\n",
    "    \n",
    "    #Zip the file then delete the temp\n",
    "    zippy.write(fileName)\n",
    "    os.remove(fileName)\n",
    "    \n",
    "    #Print progress\n",
    "    if j % 1000 == 0:\n",
    "        print(j)\n",
    "\n",
    "zippy.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-14T05:55:28.101675Z",
     "iopub.status.busy": "2022-08-14T05:55:28.101294Z",
     "iopub.status.idle": "2022-08-14T05:55:28.109822Z",
     "shell.execute_reply": "2022-08-14T05:55:28.108724Z",
     "shell.execute_reply.started": "2022-08-14T05:55:28.101643Z"
    }
   },
   "outputs": [],
   "source": [
    "os.listdir(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-14T05:54:51.291554Z",
     "iopub.status.busy": "2022-08-14T05:54:51.291117Z",
     "iopub.status.idle": "2022-08-14T05:54:51.299752Z",
     "shell.execute_reply": "2022-08-14T05:54:51.298380Z",
     "shell.execute_reply.started": "2022-08-14T05:54:51.291517Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-14T05:53:20.126983Z",
     "iopub.status.busy": "2022-08-14T05:53:20.126586Z",
     "iopub.status.idle": "2022-08-14T05:53:21.629159Z",
     "shell.execute_reply": "2022-08-14T05:53:21.627935Z",
     "shell.execute_reply.started": "2022-08-14T05:53:20.126947Z"
    }
   },
   "outputs": [],
   "source": [
    "!kaggle competitions submit -c generative-dog-images -f \"images.zip\" -m \"Made Initial Submission\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this project I walked through a finished Kaggle competition to learn how to create, train, and make predictions with a memorizer GAN. The general process was as follows:\n",
    "\n",
    "1) Import and clean data.\n",
    "\n",
    "2) Do an EDA to understand the data.\n",
    "\n",
    "3) Create a discriminator model to tell photos of dogs apart from random noise. The discriminator memorized 10,000 dogs, and so our GAN is a memorizer GAN and not a general GAN.\n",
    "\n",
    "4) Create a generator model that can do reverse convolution to transform noise into sensible images of dogs.\n",
    "\n",
    "5) Connect the genererator to the discriminator through a throttle so that the generator cannot simply mimic each individual pixel expected by the discriminator.\n",
    "\n",
    "6) Train that GAN. I ended up using slightly over 12 epochs before I got tired of waiting for marginal improvement. By 12 epochs the generator had made many very convincing dog images still with notable imperfections.\n",
    "\n",
    "7) Submit to Kaggle for a score.\n",
    "\n",
    "I learned a lot, largely thanks to user cdeotte. Many, many of the publicly available notebooks that I looked at to learn how to make a GAN reference / emulate his work. My project is no different. Thanks for the knowledge cdeotte, you have significantly contributed to my education.\n",
    "\n",
    "In the future I would like to build a general GAN, but for this project I was worried about making the shapes work. In addition general GAN's are much more computationally expensive, and Kaggle barely supplied me with enough CPU and GPU RAM to run this project. If I can find a couple nice auxillary graphics cards to program on, making a general GAN is a top priority."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliography:\n",
    "\n",
    "1) https://www.kaggle.com/code/roydatascience/introduction-to-generative-adversarial-networks\n",
    "\n",
    "2) https://www.kaggle.com/code/cdeotte/supervised-generative-dog-net \n",
    "\n",
    "3) https://stackoverflow.com/questions/3451111/unzipping-files-in-python\n",
    "\n",
    "4) https://www.kaggle.com/code/paulorzp/show-annotations-and-breeds/notebook\n",
    "\n",
    "5) https://www.kaggle.com/jesucristo/memorizer-cgan-for-dummies"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
